<!DOCTYPE html>
<html>
    <head>
        <meta name="viewport" content="width=device-width, initial-scale=0.6">
        <title>Using LLMs for SQL Generation | Notebook | Musa Haydar </title>
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Source+Serif+Pro:wght@400&display=swap" rel="stylesheet">
        <link href="https://fonts.googleapis.com/css2?family=Ubuntu+Mono&display=swap" rel="stylesheet"> 
        <link rel="stylesheet" href="../css/style.css">
        <link rel="icon" href="../images/Moon_Sprite_Big.png">
    </head>
    <body>
        <div class="notebook">
            <div class="body_header">
                <h1>Using LLMs for SQL Generation</h1>
                <div class="subtitle">Musa Haydar | May 23, 2025</div>
                <hr/>
            </div>
            <div class="about_body">
                <div class="body_text">
                    <p>I participated in two "Generative AI" hackathons at my job. I was asked to participate despite my reservations. I quipped after the first that all I learned during the hackathon is that LLMs are awful. During the second though, I think my use case was worth a little more discussion.</p>
                    <p>I'll begin with a disclaimer: LLM technology is immature, and the results are inconsistent (you probably don't need me to list countless instances of Google Search's "AI summary" being somewhat to entirely wrong!). This makes it especially pathetic how the technology is forced into every corner of consumer software, but even if the technology was good enough to justify itself, the environmental impact and widespread plagiarism are reason enough not for it to be commonplace in its present state.</p>
                    <p>Despite LLMs being known to produce frequent slop, incorrect answers, and "hallucinations," the other teams' use cases generally followed this format: feed the LLM lots of data (structured or unstructured) as context, and have it answer questions as a chatbot, perhaps with some integration (e.g. a Slack bot). This was the use case I was asked to work on in the first hackathon, and the results were bad. However, the use case I was asked to work on began with structured data, and that gave me an idea: if the answers are all in the data, why rely on the LLM to interpret them? We more or less know what the result of that will look like. The difficult part is not in interpreting the data but getting the data to the user who asks the question in a natural language.</p>
                    <p>My idea, then, was to instruct the LLM to generate SQL queries according to the question posed to it. This way, we know that any data we see in the response must be accurate (though, whether the data returned by the application answers the question asked is where the inaccuracy of the LLM poses a challenge). The application workflow was like such: the user asks a question to a chat bot, which is used by the LLM to generate a SQL query. The SQL query is then processed, run over the database, and the output is processed again before being returned to the user.</p>
                    <p>This means that the LLM was neither given access to the data nor expected to process it, as in a RAG model. Instead, we provided a small bit of JSON context to the LLM, which listed and described each of the table's columns and respective data types and format.</p>
                    <p>Of course, this meant we still observed similar issues that a general LLM faces. One example is that the model struggled to differentiate between the <span class="code_inline">report_date</span> and <span class="code_inline">report_generated_datetime</span> columns, even when the language used in the question explicitly specified whether we wanted to know about the "generated" date. As another example, when asked about a specific date on a field that contains a timestamp, the resulting <span class="code_inline">WHERE</span> clause should specify a time range, not an equals. Sometimes, the LLM even generated valid SQL that was simply not supported by the engine we were using, and that required further specification. We therefore had to be very, very specific when elaborating our system prompt, but even then there are simply no guarantees.</p>
                    <p>Running SQL generated by user prompts directly on a database is obviously a terrible idea, but this could be mitigated either by running it with read-only access, or else processing the query to ensure only queries that read the data are accepted. In our case, we provided read-only exports of the data for experimentation.</p>
                    <p>Both these experiences working with LLMs has taught me that, in the current state of the technology, it's absurd to be returning these results directly to the user. Google-searching medical questions, for instance, has never been riskier (ever since I observed Gemini's failure to describe the characters of Monte-Cristo accurately, I only search with AI disabled. This is increasingly more difficult as sub-questions in Google seem to ignore that directive). This solution we explored during the hackathon wasn't perfect either: there was no way for the user to know if the question was translated to accurate SQL without reading the SQL statement itself. But it seems that focusing the LLM on the work of translating a natural language question, rather than having it produce an answer too, is more promising in the technology's current state.</p>
                    <p>Not that it matters though: shareholders seem to fixate on the shiniest things, and solutions that abstract away thinking altogether seem to be winning out.</p>
                    <div class="thin"><hr /></div>
                    <p>This post was originally written on May 15th, two days after the hackathon had taken place. Coincidentally, a friend, to whom I had sent a draft of this article, shared with me <a href="https://cloud.google.com/blog/products/databases/techniques-for-improving-text-to-sql">a post from the Google Cloud Blog</a> that was published the following day (May 16th), and which made it to the top of Hacker News, for this very use case.</p>
                    <br/>
                <div class="links">
                    <p><a href="../notebook.html">Return</a></p>
                </div>
                <br/>       
            </div>
            <br/>
			<br/>
            <br/>
        </div>
    </body>
</html>